{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabel uncertainty\n",
    "\n",
    "Relabeling of uncertainty for SelfTrained approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import logging\n",
    "\n",
    "# CheXpert pathologies on original paper\n",
    "pathologies = ['Atelectasis',\n",
    "               'Cardiomegaly',\n",
    "               'Consolidation',\n",
    "               'Edema',\n",
    "               'Pleural Effusion']\n",
    "\n",
    "# Uncertainty policies on original paper\n",
    "uncertainty_policies = ['U-Ignore',\n",
    "                        'U-Zeros',\n",
    "                        'U-Ones',\n",
    "                        'U-SelfTrained',\n",
    "                        'U-MultiClass']\n",
    "\n",
    "\n",
    "# #####################\n",
    "# # Create a Dataset ##\n",
    "# #####################\n",
    "class UncetaintyOnlyCheXpertDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path: Union[str, None] = None,\n",
    "                 uncertainty_policy: str = 'U-Ones',\n",
    "                 logger: logging.Logger = logging.getLogger(__name__),\n",
    "                 pathologies: List[str] = pathologies,\n",
    "                 train: bool = True,\n",
    "                 resize_shape: tuple = (256, 256)) -> None:\n",
    "        \"\"\" Innitialize dataset and preprocess according to uncertainty policy.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to csv file.\n",
    "            uncertainty_policy (str): Uncertainty policies compared in the\n",
    "            original paper.\n",
    "            Check if options are implemented. Options: 'U-Ignore', 'U-Zeros',\n",
    "            'U-Ones', 'U-SelfTrained', and 'U-MultiClass'.\n",
    "            logger (logging.Logger): Logger to log events during training.\n",
    "            pathologies (List[str], optional): Pathologies to classify.\n",
    "            Defaults to 'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
    "            'Edema', and 'Pleural Effusion'.\n",
    "            transform (type): method to transform image.\n",
    "            train (bool): If true, returns data selected for training, if not,\n",
    "            returns data selected for validation (dev set), as the CheXpert\n",
    "            research group splitted.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if not (uncertainty_policy in uncertainty_policies):\n",
    "            logger.error(\n",
    "                \"Unknown uncertainty policy. Known policies: \" +\n",
    "                f\"{uncertainty_policies}\")\n",
    "            return None\n",
    "\n",
    "        split = 'train' if train else 'valid'\n",
    "        csv_path = f\"CheXpert-v1.0/{split}.csv\"\n",
    "        path = str(data_path) + csv_path\n",
    "\n",
    "        self.in_cloud = False\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        try:\n",
    "            data = pd.read_csv(path)\n",
    "            data['Path'] = data_path + data['Path']\n",
    "            logger.info(\"Local database found.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Couldn't read csv at path {path}./n{e}\")\n",
    "            try:\n",
    "                # Find files at gcp\n",
    "                project_id = 'labshurb'\n",
    "\n",
    "                storage_client = storage.Client(project=project_id)\n",
    "                self.bucket = storage_client.bucket(\n",
    "                    'chexpert_database_stanford')\n",
    "\n",
    "                blob = self.bucket.get_blob(csv_path)\n",
    "                blob.download_to_filename('tmp.csv')\n",
    "                data = pd.read_csv('tmp.csv')\n",
    "\n",
    "                self.in_cloud = True\n",
    "                logger.info(\"Cloud database found.\")\n",
    "\n",
    "            except Exception as e_:\n",
    "                logger.error(f\"Couldn't reach file at path {path}./n{e_}\")\n",
    "                quit()\n",
    "\n",
    "        data.set_index('Path', inplace=True)\n",
    "\n",
    "        # data = data.loc[data['Frontal/Lateral'] == 'Frontal'].copy()\n",
    "        data = data.loc[:, pathologies].copy()\n",
    "\n",
    "        # it will change for 15 in case of multiclass\n",
    "        label_cols = 5\n",
    "\n",
    "        data.fillna(0, inplace=True)\n",
    "\n",
    "        # U-Ignore\n",
    "        if uncertainty_policy == uncertainty_policies[0]:\n",
    "            # the only change is in the loss function, we mask the -1 labels\n",
    "            # in the calculation\n",
    "            pass\n",
    "\n",
    "        # U-Zeros\n",
    "        elif uncertainty_policy == uncertainty_policies[1]:\n",
    "            data.replace({-1: 0}, inplace=True)\n",
    "\n",
    "        # U-Ones\n",
    "        elif uncertainty_policy == uncertainty_policies[2]:\n",
    "            data.replace({-1: 1}, inplace=True)\n",
    "\n",
    "        # U-SelfTrained\n",
    "        elif uncertainty_policy == uncertainty_policies[3]:\n",
    "            logger.warning(\n",
    "                f\"Using {uncertainty_policy} uncertainty policy, \" +\n",
    "                \"make sure there are no uncertainty labels in the dataset.\")\n",
    "            return None\n",
    "\n",
    "        # U-MultiClass\n",
    "        elif uncertainty_policy == uncertainty_policies[4]:\n",
    "            #data.replace({-1: 2}, inplace=True)\n",
    "\n",
    "            one_hot_0 = [1., 0., 0.]\n",
    "            one_hot_1 = [0., 1., 0.]\n",
    "            one_hot_2 = [0., 0., 1.]\n",
    "\n",
    "            data.loc[:, pathologies] = data.map(lambda x: one_hot_0 if x == 0 else one_hot_1 if x == 1 else one_hot_2).to_numpy()\n",
    "\n",
    "            label_cols = 15\n",
    "\n",
    "        data = data[(data == -1).any(axis=1)].copy()\n",
    "\n",
    "        self.image_names = data.index.to_numpy()\n",
    "        self.labels = np.array(\n",
    "            data.loc[:, pathologies].values.tolist()\n",
    "            ).reshape((-1, label_cols))\n",
    "        self.transform = T.Compose([\n",
    "                  T.Resize(resize_shape),\n",
    "                  T.ToTensor(),\n",
    "                  T.Normalize(mean=[0.5330], std=[0.0349])\n",
    "              ])  # whiten with dataset mean and stdif transform)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[np.array, Tensor]:\n",
    "        \"\"\" Returns image and label from given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of sample in dataset.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of grayscale image.\n",
    "            torch.Tensor: Tensor of labels.\n",
    "        \"\"\"\n",
    "        if self.in_cloud:\n",
    "            img_bytes = self.bucket.blob(\n",
    "                self.image_names[index]).download_as_bytes()\n",
    "            # .download_to_filename('tmp.jpg')\n",
    "            img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "\n",
    "        else:\n",
    "            img = Image.open(self.image_names[index]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = self.labels[index].astype(np.float32)\n",
    "        return {\"pixel_values\": img, \"labels\": label}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Return length of dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.chexpert import CheXpertDataset\n",
    "\n",
    "pathologies = ['Atelectasis',\n",
    "                'Cardiomegaly',\n",
    "                'Consolidation',\n",
    "                'Edema',\n",
    "                'Pleural Effusion']\n",
    "\n",
    "\n",
    "def get_predictions(ckpts, approach, data_path, train=True):\n",
    "    dataset = UncetaintyOnlyCheXpertDataset(\n",
    "                data_path=data_path,\n",
    "                uncertainty_policy=approach,\n",
    "                train=train,\n",
    "                resize_shape=(224, 224))\n",
    "    dataloader = DataLoader(dataset, batch_size=234, shuffle=False)\n",
    "\n",
    "    models = []\n",
    "    for checkpoint in ckpts:\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            f\"../output/25092023/google/vit-base-patch16-224/{approach}/checkpoint-{checkpoint}\",\n",
    "        ).eval()\n",
    "        models.append(model)\n",
    "\n",
    "    columns = pathologies\n",
    "    if approach == 'U-MultiClass':\n",
    "        columns = [comb[1]+comb[0] for comb in itertools.product(pathologies, ['neg_', 'pos_', 'unc_'])]\n",
    "\n",
    "    general_output = []\n",
    "    labels = pd.DataFrame(columns=pd.MultiIndex.from_product([['labels'], columns]))\n",
    "\n",
    "    for i_model, model in enumerate(models):\n",
    "        multiindex = pd.MultiIndex.from_product([[f'model_{i_model}'], columns], names=['model', 'pathology'])\n",
    "        model_output = pd.DataFrame(columns=multiindex)\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                labels = pd.concat(\n",
    "                    [\n",
    "                        labels,\n",
    "                        pd.DataFrame(sample_batched['labels'], columns=pd.MultiIndex.from_product([['labels'], columns])),\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                    ignore_index=True)\n",
    "                \n",
    "                model_output = pd.concat(\n",
    "                    [\n",
    "                        model_output,\n",
    "                        pd.DataFrame(model(sample_batched['pixel_values']).logits.numpy(), columns=multiindex),\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                    ignore_index=True)\n",
    "                    \n",
    "        if len(general_output) == 0:\n",
    "            general_output = pd.merge(labels, model_output, left_index=True, right_index=True).copy()\n",
    "        else:\n",
    "            general_output = pd.merge(general_output, model_output, left_index=True, right_index=True)\n",
    "    return general_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  labels = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "C:\\Users\\hurbl\\AppData\\Local\\Temp\\ipykernel_23688\\748785343.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_output = pd.concat(\n",
      "c:\\Users\\hurbl\\OneDrive\\Área de Trabalho\\Loon Factory\\repository\\Chest-X-Ray-Pathology-Classifier\\.env\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "approach = 'U-Ignore'\n",
    "id_ckpts = [4, 6, 7, 8, 9, 10, 11, 13, 14, 15]\n",
    "data_path = r\"C:/Users/hurbl/OneDrive/Área de Trabalho/Loon Factory/repository/Chest-X-Ray-Pathology-Classifier/data/raw/\"\n",
    "\n",
    "ckpts = [\n",
    "    '1090', # 4\n",
    "    '1526', # 6\n",
    "    '1744', # 7\n",
    "    '1962', # 8\n",
    "    '2180', # 9\n",
    "    '2398', # 10\n",
    "    '2616', # 11\n",
    "    '3052', # 13\n",
    "    '3270', # 14\n",
    "    '3488', # 15\n",
    "    ]\n",
    "\n",
    "ignore_results = get_predictions(\n",
    "    ckpts,\n",
    "    approach,\n",
    "    data_path=data_path,\n",
    "    train=True)\n",
    "\n",
    "ignore_results.to_parquet(f'results/train_{approach}_for_selftrained.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ignore_results = pd.read_parquet(f'results/train_U-Ignore_for_selftrained.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "pathologies = ['Atelectasis',\n",
    "                'Cardiomegaly',\n",
    "                'Consolidation',\n",
    "                'Edema',\n",
    "                'Pleural Effusion']\n",
    "\n",
    "def get_roc_score(model_results, model_name=None):\n",
    "    number_of_models = model_results.columns.get_level_values(0).unique().str.contains('model_').sum()\n",
    "    models_names = [f\"model_{i_model}\" for i_model in range(number_of_models)] if model_name is None else list(model_name)\n",
    "    multiindex = pd.MultiIndex.from_product([models_names, pathologies], names=['model', 'pathology'])\n",
    "    \n",
    "    pred_total = pd.DataFrame(index=pd.Index(range(len(model_results))), columns=multiindex)\n",
    "\n",
    "    for pathology in pathologies:\n",
    "        pred_total.loc[:, (models_names, pathology)] = model_results.loc[:, (models_names, pathology)].map(sigmoid)\n",
    "        pred_total['Mean', pathology] = pred_total.loc[:, (models_names, pathology)].mean(axis=1)\n",
    "\n",
    "\n",
    "    return pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = get_roc_score(ignore_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['Mean'] = (pred.Mean >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:/'\n",
    "csv_path = f\"CheXpert-v1.0/train.csv\"\n",
    "path = str(data_path) + csv_path\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223405</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223406</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223409</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223410</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223411</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64085 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Atelectasis  Cardiomegaly  Consolidation  Edema  Pleural Effusion\n",
       "1              -1.0          -1.0           -1.0   -1.0              -1.0\n",
       "2               0.0           0.0           -1.0    0.0               0.0\n",
       "3               0.0           0.0           -1.0    0.0               0.0\n",
       "22              0.0           0.0           -1.0    0.0              -1.0\n",
       "25             -1.0           0.0            0.0    0.0              -1.0\n",
       "...             ...           ...            ...    ...               ...\n",
       "223405         -1.0           0.0            0.0    0.0               0.0\n",
       "223406         -1.0           0.0           -1.0    0.0               0.0\n",
       "223409         -1.0           0.0            0.0    0.0               1.0\n",
       "223410         -1.0           0.0            0.0    0.0              -1.0\n",
       "223411          0.0           0.0            0.0   -1.0               0.0\n",
       "\n",
       "[64085 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unc_data = data.loc[(data.loc[:, pathologies] == -1).any(axis=1), pathologies]\n",
    "unc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data.loc[:, pathologies] == -1).any(axis=1), pathologies] = np.where(unc_data == -1, pred['Mean'], unc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'E:/CheXpert-v1.0/train_wo_uncertainty.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20d8f220a602897de300e2db9f9927586b5377c8162b33009d0b2f369f06ba17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
