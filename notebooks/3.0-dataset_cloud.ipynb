{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to add DataPipe function name sharding_filter as it is already taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader, SubsetRandomSampler\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m \u001b[39mimport\u001b[39;00m IterableWrapper\n\u001b[1;32m     13\u001b[0m \u001b[39m# CheXpert pathologies on original paper\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pathologies \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAtelectasis\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mCardiomegaly\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mConsolidation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mEdema\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mPleural Effusion\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torchdata/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datapipes\n\u001b[1;32m     11\u001b[0m janitor \u001b[39m=\u001b[39m datapipes\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mjanitor\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torchdata/datapipes/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataChunk, functional_datapipe\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39miter\u001b[39m, \u001b[39mmap\u001b[39m, utils\n\u001b[1;32m     11\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mDataChunk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunctional_datapipe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39miter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torchdata/datapipes/iter/__init__.py:121\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaver\u001b[39;00m \u001b[39mimport\u001b[39;00m SaverIterDataPipe \u001b[39mas\u001b[39;00m Saver\n\u001b[1;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshardexpander\u001b[39;00m \u001b[39mimport\u001b[39;00m ShardExpanderIterDataPipe \u001b[39mas\u001b[39;00m ShardExpander\n\u001b[0;32m--> 121\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    122\u001b[0m     ShardingRoundRobinDispatcherIterDataPipe \u001b[39mas\u001b[39;00m ShardingRoundRobinDispatcher,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtararchiveloader\u001b[39;00m \u001b[39mimport\u001b[39;00m TarArchiveLoaderIterDataPipe \u001b[39mas\u001b[39;00m TarArchiveLoader\n\u001b[1;32m    125\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtfrecordloader\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    126\u001b[0m     TFRecordExample,\n\u001b[1;32m    127\u001b[0m     TFRecordExampleSpec,\n\u001b[1;32m    128\u001b[0m     TFRecordLoaderIterDataPipe \u001b[39mas\u001b[39;00m TFRecordLoader,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torchdata/datapipes/iter/util/sharding.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterator, Optional, TypeVar\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding\u001b[39;00m \u001b[39mimport\u001b[39;00m SHARDING_PRIORITIES\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_datapipe\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m \u001b[39mimport\u001b[39;00m IterDataPipe\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/sharding.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply_sharding\u001b[39m(\u001b[39mself\u001b[39m, num_of_instances, instance_id, sharding_group):\n\u001b[1;32m     23\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functional_datapipe\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39msharding_filter\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m---> 26\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mShardingFilterIterDataPipe\u001b[39;49;00m(_ShardingIterDataPipe):\n\u001b[1;32m     27\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m    Wrapper that allows DataPipe to be sharded (functional name: ``sharding_filter``). After ``apply_sharding`` is\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    called, each instance of the DataPipe (on different workers) will have every `n`-th element of the\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m        source_datapipe: Iterable DataPipe that will be sharded\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, source_datapipe: IterDataPipe, sharding_group_filter\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m):\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torch/utils/data/datapipes/_decorator.py:34\u001b[0m, in \u001b[0;36mfunctional_datapipe.__call__\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mcls\u001b[39m, non_deterministic) \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m     31\u001b[0m             \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m     32\u001b[0m                  \u001b[39misinstance\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, non_deterministic)):\n\u001b[1;32m     33\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`functional_datapipe` can only decorate IterDataPipe\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     IterDataPipe\u001b[39m.\u001b[39;49mregister_datapipe_as_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mcls\u001b[39;49m, enable_df_api_tracing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_df_api_tracing)\n\u001b[1;32m     35\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, MapDataPipe):\n\u001b[1;32m     36\u001b[0m     MapDataPipe\u001b[39m.\u001b[39mregister_datapipe_as_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/x-ray/Chest-X-Ray-Pathology-Classifier/env/lib/python3.8/site-packages/torch/utils/data/datapipes/datapipe.py:136\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function\u001b[0;34m(cls, function_name, cls_to_register, enable_df_api_tracing)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregister_datapipe_as_function\u001b[39m(\u001b[39mcls\u001b[39m, function_name, cls_to_register, enable_df_api_tracing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m function_name \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfunctions:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to add DataPipe function name \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as it is already taken\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(function_name))\n\u001b[1;32m    138\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mclass_function\u001b[39m(\u001b[39mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m         result_pipe \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(source_dp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mException\u001b[0m: Unable to add DataPipe function name sharding_filter as it is already taken"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import PurePath\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch import from_numpy, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "\n",
    "# CheXpert pathologies on original paper\n",
    "pathologies = ['Atelectasis',\n",
    "               'Cardiomegaly',\n",
    "               'Consolidation',\n",
    "               'Edema',\n",
    "               'Pleural Effusion']\n",
    "\n",
    "# Uncertainty policies on original paper\n",
    "uncertainty_policies = ['U-Ignore',\n",
    "                        'U-Zeros',\n",
    "                        'U-Ones',\n",
    "                        'U-SelfTrained',\n",
    "                        'U-MultiClass']\n",
    "\n",
    "######################\n",
    "## Create a Dataset ##\n",
    "######################\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path: str,\n",
    "                 uncertainty_policy: str,\n",
    "                 logger: logging.Logger,\n",
    "                 pathologies: List[str] = pathologies,\n",
    "                 transform = None,\n",
    "                 train: bool = True) -> None:\n",
    "        \"\"\" Innitialize dataset and preprocess according to uncertainty policy.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to csv file.\n",
    "            uncertainty_policy (str): Uncertainty policies compared in the original paper.\n",
    "            Check if options are implemented. Options: 'U-Ignore', 'U-Zeros', 'U-Ones', 'U-SelfTrained', and 'U-MultiClass'.\n",
    "            logger (logging.Logger): Logger to log events during training.\n",
    "            pathologies (List[str], optional): Pathologies to classify.\n",
    "            Defaults to 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', and 'Pleural Effusion'.\n",
    "            transform (type): method to transform image.\n",
    "            train (bool): If true, returns data selected for training, if not, returns data selected for validation (dev set), as the CheXpert research group splitted.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(uncertainty_policy in uncertainty_policies):\n",
    "            logger.error(f\"Unknown uncertainty policy. Known policies: {uncertainty_policies}\")\n",
    "            return None\n",
    "\n",
    "        split = 'train' if train  else 'valid'\n",
    "        path = PurePath(data_path, f\"CheXpert-v1.0/{split}.csv\")\n",
    "        try:\n",
    "            data = pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Couldn't read csv at path {path}.\\n{e}\")\n",
    "            quit()\n",
    "\n",
    "        data['Path'] = data_path + data['Path']\n",
    "        data.set_index('Path', inplace=True)\n",
    "        data = data.loc[:, pathologies].copy()\n",
    "        data.fillna(0, inplace=True)\n",
    "\n",
    "        # U-Ignore\n",
    "        if uncertainty_policy == uncertainty_policies[0]:\n",
    "            logger.error(f\"Uncertainty policy {uncertainty_policy} not implemented.\")\n",
    "            return None\n",
    "        \n",
    "        # U-Zeros\n",
    "        elif uncertainty_policy == uncertainty_policies[1]:\n",
    "            data.replace({-1: 0}, inplace=True)\n",
    "\n",
    "        # U-Ones\n",
    "        elif uncertainty_policy == uncertainty_policies[2]:\n",
    "            data.replace({-1: 1}, inplace=True)\n",
    "\n",
    "        # U-SelfTrained\n",
    "        elif uncertainty_policy == uncertainty_policies[3]:\n",
    "            logger.error(f\"Uncertainty policy {uncertainty_policy} not implemented.\")\n",
    "            return None\n",
    "\n",
    "        # U-MultiClass\n",
    "        elif uncertainty_policy == uncertainty_policies[4]:\n",
    "            # Do nothing and leave -1 as a label, but check if whole system works.\n",
    "            logger.error(f\"Uncertainty policy {uncertainty_policy} not implemented.\")\n",
    "            return None\n",
    "\n",
    "        self.image_names = data.index.to_numpy()\n",
    "        self.labels = data.loc[:, pathologies].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[np.array, Tensor]:\n",
    "        \"\"\" Returns image and label from given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of sample in dataset.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of grayscale image.\n",
    "            torch.Tensor: Tensor of labels.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.image_names[index]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = from_numpy(self.labels[index].astype(np.float32))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Return length of dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "#########################\n",
    "## Create a DataLoader ##\n",
    "#########################\n",
    "def get_dataloader(data_path: str,\n",
    "                   uncertainty_policy: str,\n",
    "                   logger: logging.Logger,\n",
    "                   batch_size: int,\n",
    "                   pathologies: List[str] = pathologies,\n",
    "                   train: bool = True,\n",
    "                   shuffle: bool = True,\n",
    "                   random_seed: int = 123,\n",
    "                   num_workers: int = 4, \n",
    "                   pin_memory: bool = True,\n",
    "                   apply_transform: bool = True,\n",
    "                   resize_shape: tuple = (320, 320)):\n",
    "    \"\"\"Get wrap dataset with dataloader class to help with paralellization, data loading order \n",
    "    (for reproducibility) and makes the code o bit cleaner.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Refer to CheXpertDataset class documentation.\n",
    "        uncertainty_policy (str): Refer to CheXpertDataset class documentation.\n",
    "        logger (logging.Logger): Refer to CheXpertDataset class documentation.\n",
    "        pathologies (List[str], optional): Refer to CheXpertDataset class documentation.\n",
    "        train (bool): Refer to CheXpertDataset class documentation.\n",
    "        shuffle (bool): Shuffle datasets (independently, train or valid).\n",
    "        random_seed (int): Seed to shuffle data, helps with reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: Data loader from dataset randomly (or not) loaded.\n",
    "    \"\"\"\n",
    "    transform = T.Compose([T.PILToTensor()])\n",
    "    if apply_transform:\n",
    "        transform = T.Compose([\n",
    "            T.PILToTensor(),\n",
    "            T.Resize(resize_shape),\n",
    "            lambda x: from_numpy(np.array(x, copy=True)).float().div(255),#.unsqueeze(0),   # tensor in [0,1]\n",
    "            T.Normalize(mean=[0.5330], std=[0.0349])\n",
    "            ]) # whiten with dataset mean and stdif transform)\n",
    "\n",
    "    dataset = CheXpertDataset(\n",
    "        data_path=data_path,\n",
    "        uncertainty_policy=uncertainty_policy,\n",
    "        pathologies=pathologies,\n",
    "        logger=logger,\n",
    "        train=train,\n",
    "        transform=transform\n",
    "        )\n",
    "    \n",
    "    indices = list(range(dataset.__len__()))\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = IterableWrapper([\"gcs://chexpert_database_stanford/\"]).list_files_by_fsspec(token='google_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"gcs://chexpert_database_stanford/CheXpert-v1.0/train.csv\",\n",
    "                   storage_options={\"token\": \"google_default\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv(\"../data/raw/CheXpert-v1.0_train.csv\")\n",
    "valid = pd.read_csv(\"../data/raw/CheXpert-v1.0_valid.csv\")\n",
    "\n",
    "pathologies = ['Atelectasis',\n",
    "               'Cardiomegaly',\n",
    "               'Consolidation',\n",
    "               'Edema',\n",
    "               'Pleural Effusion']\n",
    "\n",
    "train['ML_USE'] = 'training'\n",
    "train['GCS_FILE_PATH'] = 'gs://chexpert_database_stanford/' + train['Path']\n",
    "\n",
    "valid['ML_USE'] = 'validation'\n",
    "valid['GCS_FILE_PATH'] = 'gs://chexpert_database_stanford/' + valid['Path']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "valid.fillna(0, inplace=True)\n",
    "\n",
    "pathologies.extend(['ML_USE', 'GCS_FILE_PATH'])\n",
    "\n",
    "dataset = pd.concat([train, valid])\n",
    "dataset = dataset.loc[:, pathologies].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = dataset.replace({-1: 1}).copy()\n",
    "\n",
    "pathologies = pd.Series(\n",
    "            ['Atelectasis',\n",
    "             'Cardiomegaly',\n",
    "             'Consolidation',\n",
    "             'Edema',\n",
    "             'Pleural_Effusion'])\n",
    "\n",
    "def label(row):\n",
    "    labels = pathologies[row.astype(bool).values].values\n",
    "    if len(labels) == 0:\n",
    "        labels = ['None_of_the_above']\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "ones.rename({'Pleural Effusion': 'Pleural_Effusion'}, inplace=True, axis=1)\n",
    "ones['LABEL'] = ones[pathologies].apply(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = dataset.replace({-1: 0}).copy()\n",
    "\n",
    "pathologies = pd.Series(\n",
    "            ['Atelectasis',\n",
    "             'Cardiomegaly',\n",
    "             'Consolidation',\n",
    "             'Edema',\n",
    "             'Pleural_Effusion'])\n",
    "\n",
    "def label(row):\n",
    "    labels = pathologies[row.astype(bool).values].values\n",
    "    if len(labels) == 0:\n",
    "        labels = ['None_of_the_above']\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "zeros.rename({'Pleural Effusion': 'Pleural_Effusion'}, inplace=True, axis=1)\n",
    "zeros['LABEL'] = ones[pathologies].apply(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones[['ML_USE','GCS_FILE_PATH','LABEL']].to_csv('ones_chexpert.csv', index=False)\n",
    "zeros[['ML_USE','GCS_FILE_PATH','LABEL']].to_csv('zeros_chexpert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = dataset.copy()\n",
    "\n",
    "multi.loc[multi.Atelectasis == -1, 'Atelectasis'] = 'u_Atelectasis'\n",
    "multi.loc[multi.Cardiomegaly == -1, 'Cardiomegaly'] = 'u_Cardiomegaly'\n",
    "multi.loc[multi.Consolidation == -1, 'Consolidation'] = 'u_Consolidation'\n",
    "multi.loc[multi.Edema == -1, 'Edema'] = 'u_Edema'\n",
    "multi.loc[multi['Pleural Effusion'] == -1, 'Pleural Effusion'] = 'u_Pleural Effusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
