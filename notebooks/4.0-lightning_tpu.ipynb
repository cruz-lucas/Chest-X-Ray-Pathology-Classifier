{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "\n",
    "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl\n",
    "!pip install -r ../requirements.txt\n",
    "!gcloud init\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import io\n",
    "from pathlib import PurePath\n",
    "from typing import List, Union, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch import from_numpy, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, dataloader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import logging\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "\n",
    "# CheXpert pathologies on original paper\n",
    "pathologies = ['Atelectasis',\n",
    "               'Cardiomegaly',\n",
    "               'Consolidation',\n",
    "               'Edema',\n",
    "               'Pleural Effusion']\n",
    "\n",
    "# Uncertainty policies on original paper\n",
    "uncertainty_policies = ['U-Ignore',\n",
    "                        'U-Zeros',\n",
    "                        'U-Ones',\n",
    "                        'U-SelfTrained',\n",
    "                        'U-MultiClass']\n",
    "\n",
    "######################\n",
    "## Create a Dataset ##\n",
    "######################\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path: Union[str, None] = None,\n",
    "                 uncertainty_policy: str = 'U-Ones',\n",
    "                 logger: logging.Logger = logging.getLogger(__name__),\n",
    "                 pathologies: List[str] = pathologies,\n",
    "                 train: bool = True,\n",
    "                 resize_shape: tuple = (384, 384)) -> None:\n",
    "        \"\"\" Innitialize dataset and preprocess according to uncertainty policy.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to csv file.\n",
    "            uncertainty_policy (str): Uncertainty policies compared in the original paper.\n",
    "            Check if options are implemented. Options: 'U-Ignore', 'U-Zeros', 'U-Ones', 'U-SelfTrained', and 'U-MultiClass'.\n",
    "            logger (logging.Logger): Logger to log events during training.\n",
    "            pathologies (List[str], optional): Pathologies to classify.\n",
    "            Defaults to 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', and 'Pleural Effusion'.\n",
    "            transform (type): method to transform image.\n",
    "            train (bool): If true, returns data selected for training, if not, returns data selected for validation (dev set), as the CheXpert research group splitted.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        if not(uncertainty_policy in uncertainty_policies):\n",
    "            logger.error(f\"Unknown uncertainty policy. Known policies: {uncertainty_policies}\")\n",
    "            return None\n",
    "        \n",
    "        project_id = 'labshurb'\n",
    "\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        self.bucket = storage_client.bucket('chexpert_database_stanford')\n",
    "\n",
    "        split = 'train' if train  else 'valid'\n",
    "        csv_path = f\"CheXpert-v1.0/{split}.csv\"\n",
    "        path = str(data_path) + csv_path\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        try:\n",
    "            data = pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "              blob = self.bucket.get_blob(csv_path)\n",
    "              blob.download_to_filename('tmp.csv')\n",
    "              data = pd.read_csv('tmp.csv')\n",
    "            except:  \n",
    "              logger.error(f\"Couldn't read csv at path {path}.\\n{e}\")\n",
    "              quit()\n",
    "\n",
    "        data['Path'] = data['Path'] # data_path + \n",
    "        data.set_index('Path', inplace=True)\n",
    "\n",
    "        #data = data.loc[data['Frontal/Lateral'] == 'Frontal'].copy()\n",
    "        data = data.loc[:, pathologies].copy()\n",
    "        \n",
    "        data.fillna(0, inplace=True)\n",
    "\n",
    "        # U-Ignore\n",
    "        if uncertainty_policy == uncertainty_policies[0]:\n",
    "            data = data.loc[(data[pathologies] != -1).all(axis=1)].copy()\n",
    "        \n",
    "        # U-Zeros\n",
    "        elif uncertainty_policy == uncertainty_policies[1]:\n",
    "            data.replace({-1: 0}, inplace=True)\n",
    "\n",
    "        # U-Ones\n",
    "        elif uncertainty_policy == uncertainty_policies[2]:\n",
    "            data.replace({-1: 1}, inplace=True)\n",
    "\n",
    "        # U-SelfTrained\n",
    "        elif uncertainty_policy == uncertainty_policies[3]:\n",
    "            logger.error(f\"Uncertainty policy {uncertainty_policy} not implemented.\")\n",
    "            return None\n",
    "\n",
    "        # U-MultiClass\n",
    "        elif uncertainty_policy == uncertainty_policies[4]:\n",
    "            # Do nothing and leave -1 as a label, but check if whole system works.\n",
    "            logger.error(f\"Uncertainty policy {uncertainty_policy} not implemented.\")\n",
    "            return None\n",
    "\n",
    "        self.image_names = data.index.to_numpy()\n",
    "        self.labels = data.loc[:, pathologies].to_numpy()\n",
    "        self.transform = T.Compose([\n",
    "                  T.Resize(resize_shape),\n",
    "                  T.ToTensor(),\n",
    "                  T.Normalize(mean=[0.5330], std=[0.0349])\n",
    "              ]) # whiten with dataset mean and stdif transform)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[np.array, Tensor]:\n",
    "        \"\"\" Returns image and label from given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of sample in dataset.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of grayscale image.\n",
    "            torch.Tensor: Tensor of labels.\n",
    "        \"\"\"\n",
    "        img_bytes = self.bucket.blob(self.image_names[index]).download_as_bytes()#.download_to_filename('tmp.jpg')\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = from_numpy(self.labels[index].astype(np.float32))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Return length of dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "#########################\n",
    "## Create a DataLoader ##\n",
    "#########################\n",
    "def get_dataloader(data_path: str,\n",
    "                   uncertainty_policy: str,\n",
    "                   logger: logging.Logger,\n",
    "                   batch_size: int,\n",
    "                   pathologies: List[str] = pathologies,\n",
    "                   train: bool = True,\n",
    "                   shuffle: bool = True,\n",
    "                   random_seed: int = 123,\n",
    "                   num_workers: int = 4, \n",
    "                   pin_memory: bool = True,\n",
    "                   apply_transform: bool = True,\n",
    "                   resize_shape: tuple = (384, 384)):\n",
    "    \"\"\"Get wrap dataset with dataloader class to help with paralellization, data loading order \n",
    "    (for reproducibility) and makes the code o bit cleaner.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Refer to CheXpertDataset class documentation.\n",
    "        uncertainty_policy (str): Refer to CheXpertDataset class documentation.\n",
    "        logger (logging.Logger): Refer to CheXpertDataset class documentation.\n",
    "        pathologies (List[str], optional): Refer to CheXpertDataset class documentation.\n",
    "        train (bool): Refer to CheXpertDataset class documentation.\n",
    "        shuffle (bool): Shuffle datasets (independently, train or valid).\n",
    "        random_seed (int): Seed to shuffle data, helps with reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: Data loader from dataset randomly (or not) loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = CheXpertDataset(\n",
    "        data_path=data_path,\n",
    "        uncertainty_policy=uncertainty_policy,\n",
    "        pathologies=pathologies,\n",
    "        logger=logger,\n",
    "        train=train,\n",
    "        resize_shape=resize_shape\n",
    "        )\n",
    "    \n",
    "    indices = list(range(dataset.__len__()))\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "from torchvision.models.efficientnet import efficientnet_v2_l\n",
    "import torch\n",
    "\n",
    "from torchmetrics.classification import MultilabelAUROC, MultilabelF1Score, MultilabelPrecisionRecallCurve\n",
    "\n",
    "class LitEfficientnet(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 num_classes:int=5,\n",
    "                 lr=1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        model = efficientnet_v2_l(weights=\"DEFAULT\")\n",
    "        model.classifier[1] = torch.nn.Linear(1280, num_classes)\n",
    "        self.model = model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.auc = MultilabelAUROC(num_labels=num_classes, average=None, thresholds=None)\n",
    "        self.f1 = MultilabelF1Score(num_labels=num_classes, average=None)\n",
    "        self.pr_curve = MultilabelPrecisionRecallCurve(num_labels=num_classes, thresholds=None)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self.model(data)\n",
    "        train_loss = self.criterion(output, target)\n",
    "\n",
    "        train_auc = self.f1(output, target)\n",
    "\n",
    "        self.log_dict({\"train_loss\": train_loss, \"train_auc\": train_auc}, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self.model(data)\n",
    "        val_loss = self.criterion(output, target)\n",
    "\n",
    "        target = target.long()\n",
    "\n",
    "        val_auc = self.auc(output, target)\n",
    "        val_precision, val_recall, val_thresholds = self.pr_curve(output, target)\n",
    "        val_f1 = self.f1(output, target)\n",
    "\n",
    "        self.log_dict({\n",
    "             \"val_loss\": val_loss,\n",
    "             \"val_auc\": val_auc,\n",
    "             \"val_precision\": val_precision,\n",
    "             \"val_recall\": val_recall,\n",
    "             \"val_thresholds\": val_thresholds,\n",
    "             \"val_f1\": val_f1,\n",
    "             },\n",
    "            prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#from data import get_dataloader\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# imports the torch_xla package\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Data paths default values\n",
    "RAW_DATA_PATH = r\"/project/data/raw/\"\n",
    "CHECKPOINT_PATH = 'models/ckpt/'\n",
    "WANDB_PROJECT = 'Chest-X-Ray-Pathology-Classifier'\n",
    "\n",
    "\n",
    "def train(input_filepath: str=None,\n",
    "          uncertainty_policy: str='U-Ones',\n",
    "          config = None) -> None:\n",
    "\n",
    "    logger = logging.getLogger(__name__)   \n",
    "    gc.collect() \n",
    "\n",
    "    with wandb.init(config=config):\n",
    "        # Hyperparameters\n",
    "        BATCH_SIZE = wandb.config.batch_size\n",
    "        GRAD_ACC = wandb.config.gradient_accumulation_steps\n",
    "        RESIZE_SHAPE = (384,384)\n",
    "        LEARNING_RATE = wandb.config.learning_rate\n",
    "        EPOCHS = wandb.config.epochs\n",
    "        NUM_WORKERS = 0\n",
    "        PIN_MEMORY = True\n",
    "        NUM_CLASSES = 5\n",
    "\n",
    "        model = LitEfficientnet(lr=LEARNING_RATE)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=WANDB_PROJECT, log_model=\"all\")\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=\"tpu\",\n",
    "            devices=1,\n",
    "            #default_root_dir=CHECKPOINT_PATH,\n",
    "            callbacks=[EarlyStopping(\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            min_delta=0.01,\n",
    "                            patience=3,\n",
    "                            divergence_threshold=2.,\n",
    "                            check_on_train_epoch_end=False)], #FinetuningScheduler() \n",
    "            #precision='bf16-mixed',\n",
    "            logger=WandbLogger,\n",
    "            max_epochs=EPOCHS,\n",
    "            log_every_n_steps=100,\n",
    "            enable_progress_bar=True,\n",
    "            accumulate_grad_batches=GRAD_ACC,\n",
    "            #profiler=\"advanced\"\n",
    "            )\n",
    "\n",
    "        wandb_logger.watch(model, log=\"all\", log_freq=1, log_graph=False)\n",
    "\n",
    "        # Data loader\n",
    "        train_dataloader = get_dataloader(data_path=input_filepath,\n",
    "                                          uncertainty_policy=uncertainty_policy,\n",
    "                                          logger=logger,\n",
    "                                          train=True,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS,\n",
    "                                          pin_memory=PIN_MEMORY,\n",
    "                                          resize_shape=RESIZE_SHAPE)\n",
    "        valid_dataloader = get_dataloader(data_path=input_filepath,\n",
    "                                          uncertainty_policy=uncertainty_policy,\n",
    "                                          logger=logger,\n",
    "                                          train=False,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS,\n",
    "                                          pin_memory=PIN_MEMORY,\n",
    "                                          resize_shape=RESIZE_SHAPE)\n",
    "\n",
    "\n",
    "        wandb.log({\n",
    "            \"Uncertainty policy\": uncertainty_policy\n",
    "        })\n",
    "\n",
    "        trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)#, ckpt_path=CHECKPOINT_PATH)\n",
    "\n",
    "        wandb.run.summary[\"state\"] = \"completed\"\n",
    "        wandb.finish(quiet=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.login()\n",
    "    \n",
    "    # method\n",
    "    sweep_config = {\n",
    "        'method': 'bayes'\n",
    "    }\n",
    "\n",
    "    # hyperparameters\n",
    "    parameters_dict = {\n",
    "        'epochs': {\n",
    "            'values': [20]\n",
    "            },\n",
    "        'gradient_accumulation_steps': {\n",
    "            'values': [16, 32, 64]\n",
    "            },\n",
    "        'batch_size': {\n",
    "            'values': [4, 8]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # metric\n",
    "    sweep_metric = {\n",
    "        'name': 'val_auc',\n",
    "        'goal': 'maximize'\n",
    "    }\n",
    "\n",
    "\n",
    "    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "    sweep_config['metric'] = sweep_metric\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)\n",
    "    #sweep_id = 'sla620fp'\n",
    "\n",
    "    wandb.agent(sweep_id, train, count=20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
